{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emma Programming Guide\n",
    "\n",
    "## Preamble\n",
    "\n",
    "Make sure you have build the *emma-tutorial* project with\n",
    "\n",
    "```\n",
    "mvn compile\n",
    "```\n",
    "\n",
    "before running the commands below.\n",
    "\n",
    "### REPL Setup\n",
    "\n",
    "Uncomment the following code if you want to include a manually deployed version of Emma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// interp.resolvers() = interp.resolvers() :+ ammonite.runtime.tools.Resolver.File(\n",
    "//   \"m2\",\n",
    "//   \"/.m2/repository\",\n",
    "//   \"/[organisation]/[module]/[revision]/[artifact]-[revision].[ext]\",\n",
    "//   m2 = true\n",
    "// )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mammonite.ops._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.nio.file.Paths\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                       \n",
       "// `provided` dependencies expected by `emma-language`\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "// compiler plugin required for `@emma.lib` annotation\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                           \n",
       "\n",
       "// project classpath\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ammonite.ops._\n",
    "import java.nio.file.Paths\n",
    "\n",
    "import $ivy.`org.emmalanguage:emma-language:0.2-rc2`\n",
    "// `provided` dependencies expected by `emma-language`\n",
    "import $ivy.`com.chuusai::shapeless:2.3.2`\n",
    "import $ivy.`org.apache.hadoop:hadoop-common:2.8.0`\n",
    "import $ivy.`org.apache.hadoop:hadoop-hdfs:2.8.0`\n",
    "// compiler plugin required for `@emma.lib` annotation\n",
    "import $plugin.$ivy.`org.scalamacros:paradise_2.11.8:2.0.1`\n",
    "\n",
    "// project classpath\n",
    "interp.load.cp(pwd / up / \"emma-tutorial-library\" / \"target\" / \"classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Data Setup\n",
    "\n",
    "The following will download the [https://openflights.org]() data and import the data model (`data.openflights`) and some auxiliary functions (`lib.openflights`) used in the following examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.example.emma.tutorial.data.openflights._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.example.emma.tutorial.lib.openflights._\n",
       "\n",
       "// download https://openflights.org data\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// project imports\n",
    "import org.example.emma.tutorial.data.openflights._\n",
    "import org.example.emma.tutorial.lib.openflights._\n",
    "\n",
    "// download https://openflights.org data\n",
    "downloadOpenFlightsData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic API\n",
    "\n",
    "Emma programs require the following import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.emmalanguage.api._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.emmalanguage.api._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataBag Basics\n",
    "\n",
    "Parallel computation in Emma is represented by expressions over a generic type `DataBag[A]`. \n",
    "The type represents a distributed collection of elements of type `A` that do not have particular order and may contain duplicates.\n",
    "\n",
    "`DataBag` instances are created directly from a Scala `Seq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val squaresSeq = 1 to 42 map { x => (x, x * x) }\n",
    "val squaresBag = DataBag(squaresSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or by reading from a supported source, e.g. `CSV` or `Parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csv = CSV(delimiter = ',')\n",
    "val airports = DataBag.readCSV[Airport](file(\"airports.dat\").toString, csv)\n",
    "val airlines = DataBag.readCSV[Airline](file(\"airlines.dat\").toString, csv)\n",
    "val routes = DataBag.readCSV[Route](file(\"routes.dat\").toString, csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, a `DataBag` can be converted back to a `Seq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val squaresSeq = squaresBag.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or written to a supported file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airports.writeCSV(file(\"airports.copy.dat\").toString, csv)\n",
    "airlines.writeCSV(file(\"airlines.copy.dat\").toString, csv)\n",
    "routes.writeCSV(file(\"routes.copy.dat\").toString, csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declarative Dataflows\n",
    "\n",
    "In contrast to other distributed collection types such as Spark's `RDD` and Flink's `DataSet`, Emma's `DataBag` type is a proper monad. \n",
    "This means that joins and cross products in Emma can be declared using `for`-comprehension syntax in a manner akin to *Select-From-Where* expressions known from SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val flightsFromBerlin = for {\n",
    "  al <- airlines\n",
    "  if al.name == \"Air Berlin\"\n",
    "  ap <- airports \n",
    "  if ap.city == Some(\"Berlin\")\n",
    "  rt <- routes\n",
    "  if rt.airlineID == Some(al.id)\n",
    "  if rt.srcID == Some(ap.id)\n",
    "} yield (rt.src, rt.dst, rt.isShared)\n",
    "\n",
    "flightsFromBerlin.collect().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to comprehension syntax, the `DataBag` interface offers some combinators.\n",
    "You can combine two `DataBag[A]` instances by taking their (duplicate preserving) `union` (corresponds to `UNION ALL` clause in SQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val srcs = routes.map(_.src) \n",
    "val dsts = routes.map(_.dst)\n",
    "val locs = srcs union dsts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can eliminate duplicates with `distinct` (corresponds to the `DISTINCT` clause in SQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dupls = locs.collect().size\n",
    "val dists = locs.distinct.collect().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sample `N` elemens using [reservoir sampling](https://en.wikipedia.org/wiki/Reservoir_sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sample = routes.map(_.src).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extend all elements in a `DataBag` with a unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val indexed = routes.map(_.src).zipWithIndex.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folds\n",
    "\n",
    "The core parallel processing abstraction provided by `DataBag[A]` is a generic pattern for parallel\n",
    "collection processing called *structural recursion*.   \n",
    "\n",
    "To understand how structural recursion works, assume for a moment that `DataBag[A]` instances can be (conceptually) constructed in one of three ways: the `Empty` bag, a singleton bag `Singleton(x)`, or the union of two existing bags `Union(xs, ys)`. *Structural recursion* works on bags by\n",
    "\n",
    "1. systematically deconstructing the input `DataBag[A]` instance, \n",
    "2. replacing the constructors with corresponding user-defined functions, and \n",
    "3. evaluating the resulting expression.\n",
    "\n",
    "Formally, the above procedure can be specified second-order function called `fold` as follows.\n",
    "\n",
    "```scala\n",
    "def fold[B](zero: B)(init: A => B, plus: (B, B) => B): B = this match {\n",
    "  case Empty         => zero\n",
    "  case Singleton(x)  => init(x)\n",
    "  case Union(xs, ys) => plus(xs.fold(e)(s, u), ys.fold(e)(s, u))\n",
    "}\n",
    "```\n",
    "\n",
    "Note how `Empty` constructors are substituted by `zero` applications, `Singleton(x)` constructors are substituted by `init(x)`, and `Union(xs, ys)` by `plus(xs, ys)`.\n",
    "\n",
    "A particular combination of `zero`, `init`, and `plus` function therefore defines a specific function. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dupls = locs.fold(0L)(_ => 1L, _ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is another way to compute the number of elements of `dupls`. Note that this expression will be evaluated **in parallel**, while the version we used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dupls = locs.collect().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first converts the `DataBag` **dupls** into a `Seq[String]` and count the number of elements locally.\n",
    "\n",
    "A convenient way to bundle a specific combination of functions passed to a `fold` is through a dedicated trait.\n",
    "\n",
    "```scala\n",
    "// defined in `org.emmalanguage.api.alg`\n",
    "trait Alg[-A, B] extends Serializable {\n",
    "  val zero: B\n",
    "  val init: A => B\n",
    "  val plus: (B, B) => B\n",
    "}\n",
    "```\n",
    "\n",
    "and overload `fold` as follows:\n",
    "\n",
    "```scala\n",
    "def fold[B](zero: B)(init: A => B, plus: (B, B) => B): B = this match {\n",
    "  case Empty         => zero\n",
    "  case Singleton(x)  => init(x)\n",
    "  case Union(xs, ys) => plus(xs.fold(e)(s, u), ys.fold(e)(s, u))\n",
    "}\n",
    "```\n",
    "\n",
    "Now, we can name commonly used triples as specific `Alg` instances.\n",
    "\n",
    "```scala\n",
    "object Size extends Alg[Any, Long] {\n",
    "  val zero: Long = 0\n",
    "  val init: Any => Long = const(1)\n",
    "  val plus: (Long, Long) => Long = _ + _\n",
    "}\n",
    "```\n",
    "\n",
    "and define corresponding aliases for the corresponding `fold(alg)` calls.\n",
    "\n",
    "```scala\n",
    "def size: Long = this.fold(Size)\n",
    "```\n",
    "\n",
    "The following `DataBag` methods are actually defined as specific folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// cardinality tests\n",
    "locs.size      // counts the number of elements\n",
    "locs.nonEmpty  // checks if empty\n",
    "locs.isEmpty   // checks if not empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// based on an implicit `Ordering`\n",
    "locs.min       // minimum\n",
    "locs.max       // maximum\n",
    "locs.top(3)    // top-K\n",
    "locs.bottom(3) // bottom-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// arithmetic operations\n",
    "routes.map(_.stops).sum\n",
    "DataBag(1 to 5).product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// predicate testing\n",
    "routes.count(_.stops < 3)\n",
    "routes.forall(_.stops < 3)\n",
    "routes.exists(r => r.src == \"FRA\" && r.dst == \"SFO\")\n",
    "routes.find(r => r.src == \"FRA\" && r.dst == \"SFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// reducers\n",
    "DataBag(1 to 5).reduce(1)(_ * _)\n",
    "DataBag(1 to 5).reduceOption(_ * _)\n",
    "DataBag(Seq.empty[Int]).reduceOption(_ * _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Parallelisation\n",
    "\n",
    "To parallelise Emma code, you need to do two things\n",
    "\n",
    "1. Setup a parallel dataflow framework (Flink or Spark)\n",
    "2. set the `emmaQuote` value to the respective Emma code fragment `emma.onSpark` or `emma.onFlink`. Of course you can also inline `emmaQuote` and use the fragments directly.\n",
    "\n",
    "### Parallel Dataflow Framework Setup\n",
    "\n",
    "Run only one of the two options below.\n",
    "\n",
    "#### Option 1: Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "// `provided` dependencies expected by `emma-spark`\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// `emma.onSpark` macro\n",
    "import $ivy.`org.emmalanguage:emma-spark:0.2-rc2`\n",
    "// `provided` dependencies expected by `emma-spark`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// required spark imports\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@56525de"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// implicit Spark environment\n",
    "implicit val sparkSession = SparkSession.builder()\n",
    "    .appName(\"Emma Programming Guide\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.warehouse.dir\", Paths.get(sys.props(\"java.io.tmpdir\"), \"spark-warehouse\").toUri.toString)\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36memmaQuote\u001b[39m: \u001b[32memma\u001b[39m.\u001b[32monSpark\u001b[39m.type = org.emmalanguage.api.emma.onSpark$@5d6f7106"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val emmaQuote = emma.onSpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Flink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.emmalanguage:emma-flink:0.2-rc2`\n",
    "// import $ivy.`org.apache.spark::spark-sql:2.1.0`\n",
    "import $ivy.`org.apache.flink::flink-scala:1.2.1`\n",
    "import $ivy.`org.apache.flink::flink-clients:1.2.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.flink.api.scala.ExecutionEnvironment\n",
    "implicit val env = ExecutionEnvironment.getExecutionEnvironment\n",
    "env.getConfig.disableSysoutLogging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Include flink target classes in the classpath\n",
    "interp.load.cp(ammonite.ops.Path(org.emmalanguage.compiler.RuntimeCompiler.default.codeGenDir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val emmaQuote = emma.onFlink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quotations\n",
    "\n",
    "The enclosed code block is optimized holistically by the Emma compiler, and `DataBag` expressions are offloaded to the corresponding parallel exeuction engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.example.emma.tutorial.data.openflights._\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// override the `airports`, `routes`, and `airlines` members\n",
    "// with the `def`-based versions from `data.openflights` module\n",
    "import org.example.emma.tutorial.data.openflights._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location(Berlin-Schönefeld International Airport,52.380001068115,13.522500038147)\n",
      "Location(Berlin-Tempelhof International Airport,52.472999572753906,13.403900146484375)\n",
      "Location(Berlin-Tegel International Airport,52.5597000122,13.2876996994)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mberlinAirports\u001b[39m: \u001b[32mDataBag\u001b[39m[\u001b[32mLocation\u001b[39m[\u001b[32mString\u001b[39m]] = org.emmalanguage.api.SparkRDD@9d2a8543"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// evaluates as Spark RDD map and filter\n",
    "val berlinAirports = emmaQuote {\n",
    "  for {\n",
    "    a <- airports\n",
    "    if a.latitude > 52.3\n",
    "    if a.latitude < 52.6\n",
    "    if a.longitude > 13.2\n",
    "    if a.longitude < 13.7\n",
    "  } yield Location(\n",
    "    a.name,\n",
    "    a.latitude,\n",
    "    a.longitude)\n",
    "}\n",
    "\n",
    "berlinAirports.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Cathay Pacific,China)\n",
      "(All Nippon Airways,Japan)\n",
      "(China Airlines,Taiwan)\n",
      "(British Airways,Dominican Republic)\n",
      "(China Eastern Airlines,Taiwan)\n",
      "(XL Airways France,Dominican Republic)\n",
      "(Ryanair,Denmark)\n",
      "(Luxair,Luxembourg)\n",
      "(Turkish Airlines,Turkey)\n",
      "(Air Canada,Canada)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrs\u001b[39m: \u001b[32mDataBag\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = org.emmalanguage.api.SparkRDD@b19ffcde"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// evaluates as Spark RDD cascasde of joins\n",
    "val rs = emmaQuote {\n",
    "  for {\n",
    "    ap <- airports\n",
    "    rt <- routes\n",
    "    al <- airlines\n",
    "    if rt.srcID == Some(ap.id)\n",
    "    if rt.airlineID == Some(al.id)\n",
    "  } yield (al.name, ap.country)\n",
    "}\n",
    "\n",
    "rs.sample(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(KRK,(1,2))\n",
      "(MLA,(1,4))\n",
      "(NUE,(21,3))\n",
      "(DTM,(1,1))\n",
      "(BIO,(1,2))\n",
      "(SZG,(11,1))\n",
      "(BUD,(1,3))\n",
      "(RIX,(5,1))\n",
      "(AMM,(4,1))\n",
      "(KLX,(1,1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36maggs\u001b[39m: \u001b[32mDataBag\u001b[39m[(\u001b[32mString\u001b[39m, (\u001b[32mLong\u001b[39m, \u001b[32mLong\u001b[39m))] = org.emmalanguage.api.SparkRDD@7b68b1a2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// evaluates as Spark RDD reduceByKey\n",
    "val aggs = emmaQuote {\n",
    "  for {\n",
    "    Group(k, g) <- routes.groupBy(_.src)\n",
    "  } yield {\n",
    "    val x = g.count(_.airline == \"AB\")\n",
    "    val y = g.count(_.airline == \"LH\")\n",
    "    k -> (x, y)\n",
    "  }\n",
    "}\n",
    "\n",
    "aggs.withFilter({ case (k, (x, y)) => x > 0 && y > 0 }).sample(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Modularity\n",
    "\n",
    "To build domain-specific libraries based on Emma, enclose your function definitions in a top-level object and annotate this object with the `@emma.lib` annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@emma.lib\n",
    "object hubs {\n",
    "  def apply(M: Int) = {\n",
    "    val rs = for {\n",
    "      Group(k, g) <- ({\n",
    "        routes.map(_.src)\n",
    "      } union {\n",
    "        routes.map(_.dst)\n",
    "      }).groupBy(x => x)\n",
    "      if g.size < M\n",
    "    } yield k\n",
    "\n",
    "    rs.collect().toSet\n",
    "  }\n",
    "}\n",
    "\n",
    "@emma.lib\n",
    "object reachable {\n",
    "  def apply(N: Int)(hubs: Set[String]) = {\n",
    "     val hubroutes = routes\n",
    "       .withFilter(r => hubs(r.src) && hubs(r.dst))\n",
    "\n",
    "     var paths = hubroutes\n",
    "       .map(r => Path(r.src, r.dst))\n",
    "     for (_ <- 0 until N) {\n",
    "       val delta = for {\n",
    "         r <- hubroutes; p <- paths if r.dst == p.src\n",
    "       } yield Path(r.src, p.dst)\n",
    "       paths = (paths union delta).distinct\n",
    "     }\n",
    "\n",
    "     paths\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// override the `hubs` and `reachable` definitions from above\n",
    "// with the identical implementations in the `lib.openflights`\n",
    "// module in order to fix a bug in the type handling\n",
    "import org.example.emma.tutorial.lib.openflights._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// evaluates as Spark RDD reduceByKey\n",
    "val rs = emmaQuote {\n",
    "  reachable(2)(hubs(50))\n",
    "}\n",
    "\n",
    "rs.sample(10).foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
